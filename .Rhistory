summary(fit1)
summary(fit2)
?mtcars
?mtcars
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
plot(x,y)
First we load the load the data
```{r, echo=TRUE}
data(mtcars)
```
###MPG for Automatic and Manual Transmissions
```{r,echo=TRUE}
library(ggplot2)
library(plyr)
mtcars$am<-factor(mtcars$am)
transmissionType <- revalue(mtcars$am, c('0'="Automatic", '1'="Manual"))
ggplot(mtcars, aes(x=transmissionType, y=mpg, fill=transmissionType)) +
geom_boxplot() +
xlab("Transmission type") +
ylab("Miles per gallon") +
ggtitle("Effect of transmission type on mile per gallon")
```
```{r,echo=TRUE}
data(mtcars)
automatic<-subset(mtcars,am==0)
max(automatic$mpg)
manual_best<-subset(mtcars,am==1 & mpg>24.2)
```
###Correlation
Next we determine how strong of a correlation there is between mpg and the other vairables in the Motor Trend Cars data set:
```{r, echo=TRUE}
cor(mtcars$mpg,mtcars)
```
From these correlations we will examine the three highest correlated variables: weight, number of cylinders, and engine displacement (in decreasing order of correlation).
```{r, echo=TRUE}
par(mfrow=c(1,3))
plot(mpg~wt,data=mtcars)
abline(lm(mtcars$mpg~mtcars$wt),col="red")
plot(mpg~cyl,data=mtcars)
abline(lm(mtcars$mpg~mtcars$cyl),col="blue")
plot(mpg~disp,data=mtcars)
abline(lm(mtcars$mpg~mtcars$disp),col="green")
fit<-lm(mpg~wt+cyl+disp+am,mtcars)
summar(fit)
summary(fit)
?step
fit<-lm(mpg~.,mtcars)
summar(fit)
summary(fit)
fit<-lm(mpg~wt+cyl+disp+am,mtcars)
summay(fit)
summary(fit)
?res
?resid
plot(resid(lm(mtcars$mpg~mtcars$am),col="red"))
abline(mean(resid(lm(mtcars$mpg~mtcars$am))))
abline(mean(mtcars$am,resid(lm(mtcars$mpg~mtcars$am))))
plot(mtcars$am,resid(lm(mtcars$mpg~mtcars$am),col="red"))
plot(fit)
plot(mtcars$cyl,mtcars$disp)
plot(mtcars$cyl,mtcars$hp)
plot(mtcars$disp,mtcars$hp)
plot(mtcars$am,mtcars$wt)
?mtcars
data(mtcars)
fit<-lm(mpg~cyl)
fit<-lm(mpg~cyl,mtcars)
summary(fit)
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train_data <- read.csv(text=getURL(train_url), na.strings=c("", "NA"))
test_data <- read.csv(text=getURL(test_url), na.strings=c("", "NA"))
library(RCurl)
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train_data <- read.csv(text=getURL(train_url), na.strings=c("", "NA"))
test_data <- read.csv(text=getURL(test_url), na.strings=c("", "NA"))
View(test_data)
pml_write_files(answers)
load("rf_model.RData", verbose=TRUE)
load("test_data.RData", verbose=TRUE)
---
title: "Are you exercising correctly? Qualitative assessment of weight lifting exercises"
output:
html_document:
smart: no
pdf_document: default
---
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now
possible to collect a large amount of data about personal activity relatively
inexpensively. These type of devices are part of the quantified self movement
- a group of enthusiasts who take measurements about themselves regularly to
improve their health, to find patterns in their behavior, or because they are
tech geeks. One thing that people regularly do is quantify how much of a
particular activity they do, but they rarely quantify how well they do it. In
this project, we use data from accelerometers on the belt,
forearm, arm, and dumbell of 6 participants [@velloso2013].
They were asked to perform barbell lifts correctly and incorrectly in 5
different ways. Given data from accelerometers, the goal is to predict the class
of action which is one of the following.
- exactly according to the specification (A)
- throwing elbows to the front (B)
- lifting the dumbbell only halfway (C)
- lowering the dumbbell only halfway (D)
- throwing the hips to the front (E).
More information is available from the website here:
http://groupware.les.inf.puc-rio.br/har
(see the section on the Weight Lifting Exercise Dataset).
# Data
The training data for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
The test data are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
```{r cache=TRUE}
library(RCurl)
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train_data <- read.csv(text=getURL(train_url), na.strings=c("", "NA"))
test_data <- read.csv(text=getURL(test_url), na.strings=c("", "NA"))
```
The first column of the data is just index. We remove it from training data
frame.
```{r}
train_data$X <- NULL
```
Similarly the user and time information should not have any effect on
whether barbell lifts are performed correctly or not.
```{r}
cols_to_remove <- c("user_name", "raw_timestamp_part_1",
"raw_timestamp_part_2", "cvtd_timestamp")
for (col in cols_to_remove) {
train_data[, col] <- NULL
}
```
Many columns in the dataset have mostly missing values. We remove
features from the training and testing data that have too many missing
values, where imputing is not an option.
```{r}
NAs <- apply(train_data,2,function(x) {sum(is.na(x))})
train_data <- train_data[,which(NAs == 0)]
```
We also remove features that
don't have many missing values but have one unique value (i.e. zero
variance predictors) or have few unique values relative to the number
of samples and the ratio of frequency of the most common value to the
frequency of second most common value is large.
```{r message=FALSE}
library(caret)
nsv <- nearZeroVar(train_data)
train_data <- train_data[-nsv]
test_data <- test_data[-nsv]
```
The final set of predictors used for classification are as follows.
```{r}
names(train_data)
```
# The model
We build a random forest classifier to predict the action class. To measure
the accuracy of the model, we do 10-fold cross validation with 80:20 split, on
each fold, 80% of the data is used for training the random forest and remaining
20% is used for testing.
```{r cache=TRUE}
library(randomForest)
set.seed(1)
obs <- c()
preds <- c()
for(i in 1:10) {
intrain = sample(1:dim(train_data)[1], size=dim(train_data)[1] * 0.8, replace=F)
train_cross = train_data[intrain,]
test_cross = train_data[-intrain,]
rf <- randomForest(classe ~ ., data=train_cross)
obs <- c(obs, test_cross$classe)
preds <- c(preds, predict(rf, test_cross))
}
```
The confusion matrix for predictions on cross validation folds is given below.
```{r}
conf_mat <- confusionMatrix(table(preds, obs))
conf_mat$table
```
The proposed model seems classifying well enough. The accuracy is
`r conf_mat$overall[[1]] * 100`% and it misclassifies only few instances.
Finally, we train the random forest
with whole dataset so that the classifier can be used to predict the class of
an action, given the set of activity measurements.
```{r cache=TRUE}
model <- randomForest(classe ~ ., data=train_data)
```
# References
model <- randomForest(classe ~ ., data=train_data)
model <- randomForest(classe ~ ., data=train_data)
The first column of the data is just index. We remove it from training data
frame.
```{r}
train_data$X <- NULL
```
Similarly the user and time information should not have any effect on
whether barbell lifts are performed correctly or not.
```{r}
cols_to_remove <- c("user_name", "raw_timestamp_part_1",
"raw_timestamp_part_2", "cvtd_timestamp")
for (col in cols_to_remove) {
train_data[, col] <- NULL
}
```
Many columns in the dataset have mostly missing values. We remove
features from the training and testing data that have too many missing
values, where imputing is not an option.
```{r}
NAs <- apply(train_data,2,function(x) {sum(is.na(x))})
train_data <- train_data[,which(NAs == 0)]
```
We also remove features that
don't have many missing values but have one unique value (i.e. zero
variance predictors) or have few unique values relative to the number
of samples and the ratio of frequency of the most common value to the
frequency of second most common value is large.
```{r message=FALSE}
library(caret)
nsv <- nearZeroVar(train_data)
train_data <- train_data[-nsv]
test_data <- test_data[-nsv]
```
The final set of predictors used for classification are as follows.
```{r}
names(train_data)
```
# The model
We build a random forest classifier to predict the action class. To measure
the accuracy of the model, we do 10-fold cross validation with 80:20 split, on
each fold, 80% of the data is used for training the random forest and remaining
20% is used for testing.
```{r cache=TRUE}
library(randomForest)
set.seed(1)
obs <- c()
preds <- c()
for(i in 1:10) {
intrain = sample(1:dim(train_data)[1], size=dim(train_data)[1] * 0.8, replace=F)
train_cross = train_data[intrain,]
test_cross = train_data[-intrain,]
rf <- randomForest(classe ~ ., data=train_cross)
obs <- c(obs, test_cross$classe)
preds <- c(preds, predict(rf, test_cross))
}
```
The confusion matrix for predictions on cross validation folds is given below.
```{r}
conf_mat <- confusionMatrix(table(preds, obs))
conf_mat$table
```
The proposed model seems classifying well enough. The accuracy is
`r conf_mat$overall[[1]] * 100`% and it misclassifies only few instances.
Finally, we train the random forest
with whole dataset so that the classifier can be used to predict the class of
an action, given the set of activity measurements.
```{r cache=TRUE}
model <- randomForest(classe ~ ., data=train_data)
```
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
model <- randomForest(classe ~ ., data=train_data)
View(NAs)
?nearZeroVar
library(randomForest)
library(randomForest)
set.seed(1)
obs <- c()
preds <- c()
for(i in 1:10) {
intrain = sample(1:dim(train_data)[1], size=dim(train_data)[1] * 0.8, replace=F)
train_cross = train_data[intrain,]
test_cross = train_data[-intrain,]
rf <- randomForest(classe ~ ., data=train_cross)
obs <- c(obs, test_cross$classe)
preds <- c(preds, predict(rf, test_cross))
}
rf <- train(classe ~ .,method="rf", data=train_cross)
modelformula<-"classe ~ ."
modelformula
as.formula(modelformula)
set.seed(1)
obs <- c()
preds <- c()
for(i in 1:10) {
intrain = sample(1:dim(train_data)[1], size=dim(train_data)[1] * 0.8, replace=F)
train_cross = train_data[intrain,]
test_cross = train_data[-intrain,]
modelformula<-as.formula("classe ~ .")
rf <- train(modelformula,method="rf", data=train_cross)
obs <- c(obs, test_cross$classe)
preds <- c(preds, predict(rf, test_cross))
}
set.seed(1)
obs <- c()
preds <- c()
intrain = sample(1:dim(train_data)[1], size=dim(train_data)[1] * 0.8, replace=F)
train_cross = train_data[intrain,]
test_cross = train_data[-intrain,]
modelformula<-as.formula("classe ~ .")
rf <- train(modelformula,method="rf", data=train_cross)
?train
class(train_cross)
as.formula("classe ~ .")
set.seed(1)
intrain = sample(1:dim(train_data)[1], size=dim(train_data)[1] * 0.9, replace=F)
train_cross = train_data[intrain,]
test_cross = train_data[-intrain,]
rf <- train(as.formula("classe ~ ."), method="rf", data=train_cross)
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
library(rCharts)
load(airquality)
airquaity
data(airquality)
airquality
dTable(airquality, sPaginationType = "full_numbers")
require(devtools)
install_github('rCharts', 'ramnathv')
library(rCharts)
library(rCharts)
sudo apt-get install libcurl4-openssl-dev
sudo apt-get install openjdk-6-jdk
export LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64/server
R CMD javareconf # Let R know the configuration of Java;
install.packages(c('RJDBC', 'XLConnect', 'devtools', 'RJSONIO'))
require(devtools)
install_github('rCharts', 'ramnathv')
install.packages(c("RJDBC", "XLConnect", "devtools", "RJSONIO"))
install.packages("shiny")
deployApp()
library(shinyapp)
library(shinyapps)
deployApp()
install.packages("shiny")
library(shiny)
dTable()
x<-seq(from=0,to=20000)
for(i in 0:length(x)){
if(i<=input$changeout) {
y[i]<-i*.003+0
} else {
y[i]<-i*.001+10
}
}
x<-seq(from=0,to=20000)
for(i in 0:length(x)){
if(i<=5000) {
y[i]<-i*.003+0
} else {
y[i]<-i*.001+10
}
}
class(x)
x<-seq(from=0,to=20000)
y<-seq(from=0,to=20000)
for(i in 0:length(x)){
if(i<=5000) {
y[i]<-i*.003+0
} else {
y[i]<-i*.001+10
}
}
shiny::runApp('data_products')
shiny::runApp('data_products')
max(x)
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
?sliderInput
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
y<-array(0,c(1,20000))
y
?int
View(y)
shiny::runApp('data_products')
shiny::runApp('data_products')
x<-seq(from=0,to=20000,by=100)
y<-array(0,c(1,2000))
shiny::runApp('data_products')
x<-seq(from=0,to=20000,by=100)
y<-array(0,c(1,200))
x<-seq(from=0,to=20000,by=100)
y<-array(0,c(0,200))
x<-seq(from=0,to=20000,by=100)
y<-array(0,c(1,201))
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
?plot
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
x<-seq(from=0,to=20000,by=100)
y<-array(0,c(1,201))
for(i in 0:length(x)){
if((i*100<=input$changeout) && (max(y)<=40)) {
y[i]<-i*100*.003
} else if ((i*100<=input$changeout) && (max(y)>40)){
y[i]<-i*100*.002
} else if ((i*100>input$changeout) && (max(y)<=40)){
y[i]<-i*100*.002
} else if ((i*100>input$changeout) && (max(y)>40)){
y[i]<-i*100*.002
}
}
x<-seq(from=0,to=20000,by=100)
y<-array(0,c(1,201))
for(i in 0:length(x)){
if((i*100<=5000) && (max(y)<=40)) {
y[i]<-i*100*.003
} else if ((i*100<=5000) && (max(y)>40)){
y[i]<-i*100*.002
} else if ((i*100>5000) && (max(y)<=40)){
y[i]<-i*100*.002
} else if ((i*100>5000) && (max(y)>40)){
y[i]<-i*100*.002
}
}
plot(x,y)
ret_total<-0
for (a in 2:(length(x))) {
ret_total<-ret_total+(y[a]*100+y[a-1]*100)/2
}
text1<-ret_total
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
shiny::runApp('data_products')
library(shinyapps)
library(shiny)
setwd("data_products/")
shinyapps::setAccountInfo(name='davathin', token='90766B66F9FE0A785FEBCAE3C38C8065', secret='rv2LI5uRqQ9AdJOiabR0kLoHqKJZoprar84Pm/tg')
deployApp()
